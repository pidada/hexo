---
title: 统计学习方法法导论_1
top: false
cover: false
toc: true
katex: true
date: 2019-09-12 20:40:12
password: 
summary:
tags: 
  - ML
  - 李航
copyright: true
categories: 
  - Machine learning
  - 统计学习方法
---

### 统计学习概述

#### 统计学习的特点

> 统计学习是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测和分析的一门学科。**统计学习也称之为统计机器学习**，统计学习的主要特点是：

- 统计学习以计算机及网络为平台，是建立在计算机及网络之上的
- 统计学习以数据为研究对象，是数据驱动的学科
- 统计学习的目的是对数据进行预测和分析
- 统计学习以方法为中心，统计学习方法构建模型进行预测和分析
- 统计学习是以概率论、统计学、信息论等多个领域的交叉学科理论



<!--  MORE -->

#### 统计学习对象

统计学习的对象是数据`data`。统计学中的数据通常是以变量或者变量组来表示数据。数据分为`连续型`和`离散型`，书中主要讨论的是离散型数据。

- 从数据出发，提取数据中的特征
- 抽象出数据中的模型，发现数据中的知识
- 将模型应用到数据的分析和预测中去

#### 统计学习目的

统计学习的目的是对数据进行预测和分析，特别是对新数据的预测和分析，通过构建概率模型来实现。

#### 统计学习方法

统计学习的方法是基于数据构建概率统计模型来对数据进行预测和分析。统计学习分为：

- 监督学习 `supervised learning`
- 无监督学习 `unsupervised learning`
- 强化学习 `reinforcement learning`

实现统计学习的步骤：

1. 给定一个有限的训练数据集合
2. 构建学习模型的集合
3. 确定模型选择的准则，即学习的策略
4. 实现求解最优模型的算法
5. 通过学习方法来选择最优解
6. 利用学习的最优解模型来对数据进行预测和分析

### 统计学习基本分类

#### 监督学习

> 监督学习 `supervised learning`：从标注数据中学习预测模型的机器学习问题。

- 在监督学习中将输入和输出所有可能值组成的集合称之为`输入空间`和`输出空间`。
- 输入和输出空间可以是有限元素的集合，也可以值整个欧式空间。通常输出空间远远小于输入空间
- 实例：`isintance`，每个具体的输入。通常由特征向量`feature vector`表示。
- 特征空间：特征向量存在的空间。有时候假设输入空间和特征空间为同一个空间
- 表示方法：输入变量`X`，输出变量`Y`；输入变量的取值`x`，输出变量的取值`y`

输入实例`x`的特征向量记为：
$$
x = (x^{(1)},x^{(2)},...,x^{(i)},...,x^{(n)})^T
$$
其中$x^{(i)}$表示`x`的第`i`个特征；$x_i$表示多个输入变量中的第`i`个变量：
$$
x_i = {(x_i^{(1)},x_i^{(2)},...x_i^{(j)},...x_i^{(n)})}^T
$$


- 监督学习从训练集`training data`中学习模型，对测试数据`test data`进行预测。训练数据由输入和输出成对组成，训练集通常表示为：
  $$
  T={(x_1,y_1), (x_2,y_2),...(x_i,y_i),...,(x_N,y_N)}
  $$
  
- 输入和输出对又称之为样本或样本点`sample`：(x<sub>i</sub>, y<sub>i</sub>),i=1,2,3...,N

- x<sub>i</sub>是实例或者输入，y<sub>i</sub>是输出

**三大问题**

- 回归问题：输入和输出均为连续变量的预测问题，比如预测房价问题
- 分类问题：输出变量为有限个离散变量的预测问题
- 标注问题：输入和输出变量均为变量序列的预测问题

> 监督学习利用训练数据学习一个模型，再用模型对测试样本进行预测。在这个过程通常是人工给出标注的训练数据集，所以称之为监督学习。监督学习分为学习和预测。

对于给定的输入x<sub>i</sub>，一个具体的模型可以产生一个输出f(x<sub>i</sub>)；数据集中真实的输出是y<sub>i</sub>。如果两个相差很大，说明模型的预测能力很差。学习系统即算法需要不断地尝试，选择出最好的模型。

#### 无监督学习

无监督学习是指从无标注数据中学习预测模型的机器学习问题。无监督学习的本质是学习数据中的统计规律和潜在结构。

- 无监督学习通常使用大量无标注数据学习或者训练，每个样本是个实例

- 训练数据表示为
  $$
  U = \{x_1, x_2, ...x_i,...,x_N\}
  $$
  
- 无监督学习的学习过程中，学习系统从训练数据集学习，得到一个最优模型，表示为：$z=\hat{g}(x)$或者条件概率分布$\hat{P}(z|x)$或者条件概率分布$\hat{P}(x|z)$



#### 强化学习

强化学习`reinforcement learning` 是指在智能系统在与环境的连续互动中学习最优行为的机器学习问题。强化学习的本质是学习最优的序贯决策。

- 智能系统从环境中观测到一个状态(state)s<sub>t</sub>和一个奖励(reward)r<sub>t</sub>，同时采取动作(action)a<sub>t</sub>
- 智能系统的目标不是短期奖励的最大化，而是长期累积奖励的最大化。
- 强化学习在不断地试错过程中，达到学习最优策略的目的。

**强化学习的马尔可夫决策过程是状态、奖励、动作序列上的随机过程，包含5个要素：**

- `S`是有限状态`state`的集合

- `A`是有限动作的`action`的集合

- `P`是状态转移概率`transition probability`的函数：
  $$
  P(s'|s,a)=P(s_{t+1}=s'|s_t=s, a_t=a)
  $$
  
- `r`是奖励函数`reward function`：
  $$
  r(s,a)=E(r_{t+1}|s_t=s, a_t=a)
  $$
  
- $\gamma$是衰减系数`discount factor`：
  $$
  {\gamma}\in[0,1]
  $$
  

#### 半监督学习和主动学习

> 半监督学习`semi-supervised learning` 是指利用标注数据和未标注数据学习预测模型的机器学习问题。

- 少量标注数据，大量未标注数据，因为标注数据的构建需要人工，成本高
- 通过未标注的数据辅助标注数据，通过监督学习，以较低的成本达到较好的学些效果

------

> 主动学习`active learning `是机器不断地给出实例进行标注，然后利用标注的数据进行预测学习。

- 通常的监督学习使用给定的标注数据，往往是随机得到的，看做是被动学习过程。
- 主动学习以较小的标注代价，达到较好的学习效果

**半监督学习和主动学习更接近监督学习**

------

### 按照模型分类

#### 概率和非概率模型

- 概率模型`probabilistic model`：决策树、朴素贝叶斯、隐马尔可夫模型、条件随机场、高斯混合模型等

  - 监督学习中是条件概率分布形式：$P(y|x)$

  - 无监督学习中是条件概率分布形式：$P(z|x)$或者$P(x|z)$

- 非概率模型`non-probabilistic model`：感知机、支持向量机、、k均值、神经网络等

  - 监督学习中是函数形式：
    $$
    y=f(x)
    $$
    `x`是输入，`y`是输出

  - 无监督学习中是函数形式：
    $$
    z=g(x)
    $$
    `x`是输入，`z`是输出

**在监督学习中，概率模型是生成型，非概率模型是判别模型**

> 条件概率分布最大化可以得到函数，函数归一化后可以得到条件概率分布

#### 线性和非线性模型

统计学习模型，尤其是非概率模型，可以分为概率模型`linear model`和非概率模型`non-linear model`。

- 线性模型：感知机、线性支持向量机`SVM`、`k`近邻、`k-means`
- 非线性模型：核函数支持向量机、`AdaBoost`、神经网络

#### 参数化和非参数化模型

参数模型假设模型参数的维度固定，模型可以是有限维度的完全刻画；非参数化模型假设模型的参数是不固定或者无穷大的。

- 参数化模型：感知机、朴素贝叶斯、逻辑斯蒂回归、k均值、高斯混合模型
- 非参数化模型：决策树、支持向量机、k近邻、提升树`AdaBoost`等

#### 按照算法分类

统计学习根据算法可以分为在线学习`online learning`和批量学习 `batch learning`。

- 在线学习：每次接受一个样本，进行预测，之后学习模型并且不断地重复此过程
- 批量学习：一次性接受所有的数据，学习模型，之后进行预测。

**在线学习通常比批量学习更难，因为在线学习很难学到准确率更高的模型，因为每次模型更新中，可以利用的数据是有限的**

------

### 统计三要素

统计学习方法都是由模型、策略和算法构成的，简单地表示为：$$方法 = 模型+策略+算法$$

#### 模型

在监督学习的过程中，模型就是所要学习的条件概率分布或者决策函数。模型的假设空间（hypothesis space）包含所有可能的条件概率分布或者函数。
假设空间的模型一般有无穷多个。

1. 假设空间用${\Gamma}$表示，假设空间可以定义为决策函数的集合：$${\Gamma}={f|Y=f(X)}$$。

- X和Y是定义在输入和输出空间上的变量
- ${\Gamma}$通常是由一个参数向量决定的`函数族`：$${\Gamma}=\{f|Y=f_\theta(X), \theta\in{R^n}\}$$
- 参数${\theta}$取值于n维欧式空间${R^n}$，称为参数空间 `parameter space`

2. 假设空间定义为条件概率的集合：
   $$
   {\Gamma}=\{P|P(Y|X)\}
   $$
   

- X和Y是定义在输入和输出空间上的变量

- ${\Gamma}$通常是由一个参数向量决定的`条件概率族`：
  $$
  {\Gamma}=\{P|P_\theta {(Y|X)}, \theta \in{R^n}\}
  $$
  
- 参数${\theta}$取值于n维欧式空间${R^n}$，称为参数空间 `parameter space`

#### 策略

监督学习是在假设空间${\Gamma}$中选取迷行f作为决策函数，给定输入`X`，由`f(X)`给出相应的输出`Y`，这个输出的预测值f(X)和真实值之间可能存在误差，用一个损失函数`loss function` 或者代价函数`cost function` 来度量预测错误的程度。损失函数记为：$L(Y,f(X))$

统计学中常用的损失函数有：

1. `0-1`损失函数

$$
L(Y,f(X))=
\begin{cases}
1,& \text Y\neq f(X)\\
0,& \text{Y = f(X)}
\end{cases}
$$

2. 平方损失函数
   $$
   L(Y,f(X))=(Y-f(X))^2
   $$
   
3. 绝对损失函数
   $$
   L(Y,f(X))=|Y-f(X)|
   $$
   
4. 对数损失函数或者似然损失函数
   $$
   L(Y,P(Y|X)) = -logP(Y|X)
   $$
   

**损失函数值越小，模型的性能就越好**

#### 什么是期望风险

输入和输出是随机变量，遵循联合概率分布`P(X,Y)`，`损失函数的期望值`为：
$$
\begin{align}
R_{exp}(f)
& = E_P[L(Y,f(X))]\\
& = \int_{X{\times}Y}{L(y, f(x))P(x,y)}{\rm d}x{\rm d}y
\end{align}
$$
称之为风险函数`risk function`或者期望损失`excepted loss`

- 学习的目标就是选择期望风险最小的模型
- 联合概率分布P(X,Y)是未知的，$R_{exp}(f)$不能直接计算
- 期望风险最小学习模型--->联合分布未知---->监督学习是`病态`问题

#### 什么是经验风险

在给定的训练数据集：
$$
T={(x_1,y_1), (x_2,y_2),...(x_i,y_i),...,(x_N,y_N)}
$$
模型$f(Y)$关于训练集的`平均损失函数`称为经验风险`empirical risk`或者经验损失`empirical loss`，记作：$R_{emp}$_
$$
R_{emp}f=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$


> 期望风险R<sub>exp</sub>(f)是模型关于联合分布的期望损失，经验风险R<sub>emp</sub>(f)是模型关于训练样本数据集的平均损失。

- 根据大数定律，当样本容量N--->$\infty$，二者近似相等。

- 联想到用经验风险估计期望风险；现实中样本容量小，很难估计，需要对经验风险进行矫正

- 监督学习的两个策略：

- 经验风险最小化`ERM`，`empirical risk minimizetion`
  $$
  R_{erm}(f)=\mathop{min}\limits_{f\in \Gamma}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
  $$
  
- 结构风险最小化`SRM`，`structured risk minimization`$$R_{srm}(f)=\mathop{min}\limits_{f\in \Gamma}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J()$$

##### 经验风险最小

 当样本容量足够大的时候，经验风险最小化能够达到很好的效果。极大似然估计就是经验风险最小化的例子。**当模型是条件概率分布，损失函数是对数损失函数的时候，经验风险最小化等价于极大似然估计。**经验风险定义：$$R_{erm}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$经验风险的最优解问题：
$$
R_{erm}(f)=\mathop{min}\limits_{f\in \Gamma}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))
$$
 当样本容量很小，学习效果不好，会产生过拟合`over-fitting`的现象。

##### 结构风险最小化

 为了防止经验风险最小化，提出了结构风险最小化，结构风险是在经验风险的基础上加了模型复杂度的正则化项`regularizer`或罚项`penalty term`。结构风险的定义是
$$
\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J()
$$
结构风险最小化的策略认为，结构风险最小化的模型就是最有的模型，求解最优模型，就是求解做优化问题：
$$
R_{srm}(f)=\mathop{min}\limits_{f\in \Gamma}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J()
$$
**贝叶斯估计中最大后验概率估计就是结构风险最小化的栗子。**模型是条件概率分布，损失函数是对数损失函数，模型复杂度由模型的先验概率表示，结构风险最小化就是最大后验概率估计。

#### 算法

> 算法是指学习模型中的具体计算方法。统计学习基于训练数据集，根据学习策略，从假设空间中选择最优模型，最后需要考虑用什么算法来求解最优模型



<CENTER><span style='font-size:2.5rem'>Stay Foolish Stay Hungry</span></CENTER>