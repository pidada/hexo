---
title: 基于决策树的红酒分类可视化
top: false
cover: false
toc: true
mathjax: true
date: 2019-09-22 23:29:41
password:
summary:
copyright: true
tags: 
  - 可视化
  - sklearn
  - pandas
  - matplotlib
categories:
  - Machine learning
---

> 本文中讲解是的利用决策树的方法将`sklearn`中自带的红酒数据进行划分和可视化显示，学习决策树的几个重要参数。

### 决策树在sklearn的应用

决策树`Decision Tree`是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规
则，并用树状图的结构来呈现这些规则，以解决分类和回归问题 。

#### 解决两个重点问题

- 如何从数据中找出最佳节点和最佳分枝
- 如何让决策树停止生长，防止过拟合

#### sklearn中的决策树

决策树相关的类都在`tree`模块下面，总共5个
![](https://s2.ax1x.com/2019/09/22/upNeCn.png)



<!-- MORE -->

### 建模的基本流程

- 实例化
- 拟合`fit`
- 计算准确度`score`

```
from sklearn import tree              # 导入需要的模块
 
clf = tree.DecisionTreeClassifier()   # 实例化  
clf = clf.fit(X_trian, y_train)       # 用训练数据训练模型
result = clf.score(X_test, t_test)    # 导入测试数据集，从接口中调用需要的信息
```

![](https://s2.ax1x.com/2019/09/22/uCQuuQ.png)



### 重要参数

决策树算法中所有的参数为

```python
class sklearn.tree.DecisionTreeClassifier (criterion=’gini’, splitter=’best’, max_depth=None,
                                           min_samples_split=2, min_samples_leaf=1,
                                           min_weight_fraction_leaf=0.0, max_features=None,
                                           random_state=None, max_leaf_nodes=None,
                                           min_impurity_decrease=0.0, min_impurity_split=None,
                                           class_weight=None, presort=False)
```



1.`criterion`
用来确定不纯度的计算方法有两种，不纯度越低越好

- 信息熵`entropy`，实际上是信息增益

- 基尼系数`gini `（默认）
  ![](https://s2.ax1x.com/2019/09/22/upIrdg.png)

  

  

  **二者比较**

- 信息熵对不纯度更加敏感

- 信息熵更慢些，存在对数运算

- 数据维度大，噪音很大使用基尼系数

- 当拟合程度不够的时候，使用基尼系数



### 导入模块和库

```python
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import tree    # tree模块
from sklearn.datasets import load_wine  # 导入红酒数据
from sklearn.model_selection import train_test_split  # TTS模块
```

### 数据生成和信息查看

```python
wine = load_wine()   # 实例化红酒数据
wine.data
```

```
array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,
        1.065e+03],
       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,
        1.050e+03],
       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,
        1.185e+03],
       ...,
       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,
        5.600e+02]])
```

```python
wine.data.shape
# 结果：178个样本，13个属性
(178, 13)
# 3种分类
wine.target  
```

```
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2])
```

### 重点：如何将样本数据集和输出标签生成表格形式

```python
pd.concat([pd.DataFrame(wine.data), pd.DataFrame(wine.target)], axis=1)
```


![](https://s2.ax1x.com/2019/09/23/uCr33F.png)

-----

```PYTON
wine.feature_names   # 13个属性名称
```

```python
# 结果
['alcohol',
 'malic_acid',
 'ash',
 'alcalinity_of_ash',
 'magnesium',
 'total_phenols',
 'flavanoids',
 'nonflavanoid_phenols',
 'proanthocyanins',
 'color_intensity',
 'hue',
 'od280/od315_of_diluted_wines',
 'proline']
```

```python
wine.target_names  # 标签的3个分类
array(['class_0', 'class_1', 'class_2'], dtype='<U7')
```

```python
Xtrain, Xtest, ytrain, ytest = train_test_split(wine.data, wine.target, test_size=0.3)   # 随机划分数据
Xtrain.shape
(124, 13)
```

```python
ytrain
```

```
array([1, 1, 0, 1, 1, 2, 1, 1, 1, 2, 0, 0, 2, 0, 1, 0, 0, 0, 1, 1, 1, 0,
       0, 1, 1, 0, 1, 2, 2, 2, 0, 2, 0, 0, 2, 0, 1, 0, 0, 0, 2, 1, 0, 1,
       2, 1, 0, 0, 1, 2, 0, 1, 1, 0, 0, 0, 1, 2, 2, 2, 1, 1, 1, 1, 1, 2,
       0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 2, 2, 1, 1, 2, 0, 2, 2, 2, 1, 0,
       2, 0, 2, 0, 2, 1, 1, 0, 1, 0, 1, 2, 1, 0, 1, 1, 1, 0, 2, 2, 1, 0,
       0, 1, 2, 0, 2, 0, 2, 0, 0, 1, 1, 2, 0, 0])
```

### 建模过程

```python
clf = tree.DecisionTreeClassifier(criterion="entropy")  
clf = clf.fit(Xtrain, ytrain)
score = clf.score(Xtest, ytest)    # 返回预测的准确度 
score
0.9259259259259259
```

```python
import os   # 画图的时候一定要加上路径
os.environ["PATH"] += os.pathsep + 'D:/Tools/graphviz-2.38/release/bin'
```



### 画图

```python
feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类',
                '花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']

import graphviz
dot_data = tree.export_graphviz(clf
                               ,feature_names = feature_name
                               ,class_names = ["琴酒","雪莉","贝尔摩德"]
                               ,filled = True    # 是否填充颜色
                               ,rounded = True)  # 框的形状

graph = graphviz.Source(dot_data)
graph
```



![](https://s2.ax1x.com/2019/09/23/uCBTJS.png)



### 结果信息

```python
clf.feature_importances_   # 使用特征的数量的重要性
```

```
array([0.02366882, 0.04362795, 0.        , 0.        , 0.        ,
       0.        , 0.        , 0.        , 0.        , 0.16528255,
       0.        , 0.43075257, 0.33666811])
```

```python
[*zip(feature_name,clf.feature_importances_)]  # 将使用的特征和名称进行一一对应
```

```
[('酒精', 0.023668823820059623),
 ('苹果酸', 0.04362794529024377),
 ('灰', 0.0),
 ('灰的碱性', 0.0),
 ('镁', 0.0),
 ('总酚', 0.0),
 ('类黄酮', 0.0),
 ('非黄烷类酚类', 0.0),
 ('花青素', 0.0),
 ('颜色强度', 0.16528255077367338),
 ('色调', 0.0),
 ('od280/od315稀释葡萄酒', 0.4307525705140722),
 ('脯氨酸', 0.3366681096019511)]

```

- `random_state`：设置随机模式的参数，默认是`None`，高维数据表现更明显
- `splitter`：有两个参数供选择
  - `best`：默认，每次选择更重要的属性进行分类
  - `random`：保证选择特征的随机性，树会更深更大，降低对训练数据的拟合

```python
clf = tree.DecisionTreeClassifier(criterion="entropy"
                                  ,random_state=50   # 设置随机模式，保证结果不变
                                  ,splitter="random"  
                                  )   
clf = clf.fit(Xtrain, ytrain)
score = clf.score(Xtest, ytest)    # 返回预测的准确度 
```

```python
feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类',
                '花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']

import graphviz
dot_data = tree.export_graphviz(clf
                               ,feature_names = feature_name
                               ,class_names = ["琴酒","雪莉","贝尔摩德"]
                               ,filled = True    # 是否填充颜色
                               ,rounded = True)  # 框的形状

graph = graphviz.Source(dot_data)
graph
```



![](https://s2.ax1x.com/2019/09/23/uCDkLR.png)



### 剪枝参数

过拟合：在训练数据集上表现的很好，在测试数据集上却很差

- `max_depth`
  限制树的最大深度，超过设定深度的树枝全部剪掉
- `min_samples_leaf & min_samples_split`
  `min_samples_leaf`限定，一个节点在分枝后的每个子节点都必须包含至少`min_samples_leaf`个训练样本
  `min_samples_split`限定，一个节点必须要包含至少`min_samples_split`个训练样本，这个节点才允许被分枝，否则分枝就不会发生。

```python
clf = tree.DecisionTreeClassifier(criterion="entropy"
                                  ,random_state=50   # 设置随机模式，保证结果不变
                                  ,splitter="random"  
                                  # 可以调节3个参数，比较每次的得分大小
                                  ,max_depth=3   
                                  ,min_samples_leaf=10
                                  ,min_samples_split=10
                                  )   
clf = clf.fit(Xtrain, ytrain)

dot_data = tree.export_graphviz(clf
                               ,feature_names = feature_name
                               ,class_names = ["琴酒","雪莉","贝尔摩德"]
                               ,filled = True    # 是否填充颜色
                               ,rounded = True)  # 框的形状

graph = graphviz.Source(dot_data)
graph
```



![](https://s2.ax1x.com/2019/09/23/uCDnJO.png)



```python
score = clf.score(Xtest, ytest)    # 返回预测的准确度 
score
0.7777777777777778
```

- `max_features`
  - 限制分枝是考虑的特征个数，超过限制的个数直接舍弃掉
  - 限制高维数据的过拟合剪枝参数，方法暴力
- `min_impurity_decrease`
  - 限制信息增益的大小
  - 小于设置值不会发生分枝

```python
# 学习曲线

test = []
for i in range(10):
    clf = tree.DecisionTreeClassifier(criterion="entropy"
                                  ,random_state=50   # 设置随机模式，保证结果不变
                                  ,splitter="random"  
                                  ,max_depth=i+1
#                                   ,min_samples_leaf=10
#                                   ,min_samples_split=10
                                  )   
    clf = clf.fit(Xtrain, ytrain)
    score = clf.score(Xtest, ytest)    # 返回预测的准确度 
    test.append(score)
plt.plot(range(1,11), test, color="red", label="max_depth")
plt.legend()
plt.show()

```

![](https://s2.ax1x.com/2019/09/23/uCDuWD.png)

### 重要属性和接口

```python
# 测试样本所在的叶子节点的索引
clf.apply(Xtest)
```

```
array([ 6,  7,  6, 18, 18,  6, 12, 16, 16,  9,  7, 16, 18,  7,  5, 12, 14,
       18,  7,  6,  7,  6, 12,  7, 18,  9,  5,  7,  5, 16, 12,  6,  7,  5,
       14, 18,  9, 12,  6,  9,  7,  9, 16, 12, 14, 12,  7,  6, 18,  5, 14,
       18,  7, 12], dtype=int64)
```

```python
#返回分类测试样本的分类或者回归结果
clf.predict(Xtest)
```

```
array([1, 2, 1, 0, 0, 1, 1, 0, 0, 1, 2, 0, 0, 2, 2, 1, 1, 0, 2, 1, 2, 1,
       1, 2, 0, 1, 2, 2, 2, 0, 1, 1, 2, 2, 1, 0, 1, 1, 1, 1, 2, 1, 0, 1,
       1, 1, 2, 1, 0, 2, 1, 0, 2, 1])
```

一个属性：`feature_importances`

四个接口：`fit，score，apply，predict`